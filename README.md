# my-foundation-models

In this repository, I aim to document the useful resources of foundational models for my work.

## Language foundation models
- Chat completion, text generation: GPT-4
- Text embeddings: [OpenAI API embeddings](https://platform.openai.com/docs/guides/embeddings), [CLIP](https://github.com/openai/CLIP) and [OpenCLIP](https://github.com/mlfoundations/open_clip), [T5](https://github.com/google-research/text-to-text-transfer-transformer)

## Vision foundation models
- Class-agnostic segmentation models: SAM, [HQ-SAM](https://github.com/SysCV/sam-hq)
- ImageNet22k trained: SwinTransformer?
- Semi-supervised models: MAE, DINOv2

  
## Vision-Language models

- CLIP, [DiHT](https://github.com/facebookresearch/diht)?
- Image tag generation: [RAM, RAM++](https://github.com/xinyu1205/recognize-anything)
- Region-level grounding model: [GLaMM](https://github.com/mbzuai-oryx/groundingLMM), GroundingDINO, GroundingSAM

- VQA, caption generation: BLIP2, CaSED, LLaVA
